Mount The drive on Google Collab Notebook and import data set

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')


!ls "/content/drive/MyDrive/leaf_dis_dataset/leaf_di_dataset/Plant_images"

# Import necessary libraries
import numpy as np # working with arrays
import pandas as pd # data manipulation
import matplotlib.pyplot as plt # create a plotting area ,fig
from matplotlib.image import imread
import cv2 #loads img from specified file
import random # loads random module b/w 0.0-1.0
import os
from os import listdir
from PIL import Image
import tensorflow as tf
from keras.preprocessing import image #utility for working with data  actions
from tensorflow.keras.utils import img_to_array, array_to_img
from keras.optimizers import Adam #gradient descent method that is based on adaptive estimation of first-order and second-order moments.
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Flatten, Dropout, Dense
from sklearn.model_selection import train_test_split
from keras.models import model_from_json # used to load a saved model architecture from a JSON file
from keras.utils import to_categorical#int labels to one hot encoded vectors


print(tf. __version__)

Step 3 Visualizing the Images and Resize Images

# Plotting 12 images to check dataset
#Now we will observe some of the images that are their in our dataset. We will plot 12 images here using the matplotlib library.
plt.figure(figsize=(12,12)) # height and width in inches
path = "/content/drive/MyDrive/leaf_dis_dataset/leaf_di_dataset/Plant_images/Potato___Early_blight"
for i in range(1,17):
    plt.subplot(4,4,i) #creates a subplot within the 4x4 grid layout.
    plt.tight_layout() # automatically adjust spacing between subplots, ensuring they fit  and avoid overlapping.
    rand_img = imread(path +'/'+ random.choice(sorted(os.listdir(path))))
    plt.imshow(rand_img)
    plt.xlabel(rand_img.shape[1], fontsize = 10)#sets x label of subplot as width of image
    plt.ylabel(rand_img.shape[0], fontsize = 10)#height of image

from google.colab import drive
drive.mount('/content/drive')

**Step 4 Convert images to NumPy array and normalize them**



##Converting Images file  to numpy array
def convert_image_to_array(image_dir):
    try:
        image = cv2.imread(image_dir) # returns numpy array represnting img
        if image is not None :
            image = cv2.resize(image, (256,256))
            return img_to_array(image)
        else :
            return np.array([])
    except Exception as e:
        print(f"Error : {e}")
        return None

dir = "/content/drive/MyDrive/leaf_dis_dataset/leaf_di_dataset/Plant_images"
root_dir = listdir(dir)
image_list, label_list = [], [] #store the image arrays and corresponding labels
all_labels = ['Corn-Common_rust', 'Potato-Early_blight', 'Tomato-Bacterial_spot']
binary_labels = [0,1,2]
#used to track the current label index.
temp = -1

# Reading and converting image to numpy array
#Now we will convert all the images into numpy array.

for directory in root_dir:
  plant_image_list = listdir(f"{dir}/{directory}")
  temp += 1
  for files in plant_image_list:
    image_path = f"{dir}/{directory}/{files}"
    image_list.append(convert_image_to_array(image_path)) #contains NumPy array of each image
    label_list.append(binary_labels[temp])


**Step 5 Visualize class Count and check for class imbalance**

# Visualize the number of classes count
label_counts = pd.DataFrame(label_list).value_counts()
label_counts.head()

#it is a balanced dataset as you can see

#Next we will observe the shape of the image.
image_list[0].shape

#Checking the total number of the images which is the length of the labels list.
label_list = np.array(label_list)
label_list.shape

**Step 6 Splitting dataset into train,validate and test sets**

x_train, x_test, y_train, y_test = train_test_split(image_list, label_list, test_size=0.2, random_state = 10)

#Now we will normalize the dataset of our images. As pixel values ranges from 0 to 255 so we will divide each image pixel with 255 to normalize the dataset.
x_train = np.array(x_train, dtype=np.float16) / 225.0
x_test = np.array(x_test, dtype=np.float16) / 225.0
x_train = x_train.reshape( -1, 256,256,3)
x_test = x_test.reshape( -1, 256,256,3)

**Step 7 Perform One Hot Encoding on Target Variable**

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

**Step 8 Creating model Architecture ,compile model and fit it using training data**

We will create a network architecture for this model.We have used different types of layers according to their features namely Conv_2d(it is used to create a convolution kernel that is convoluted with input layer to produce the output tensor),Max_pooling2d(it is a downsampling technique which takes out the maximum value over the window defined by poolsize),flatten(it flattens input and creates 1d output),Dense(this layer produce output as the dot product of input and kernel)

model = Sequential()
model.add(Conv2D(32, (3, 3), padding="same",input_shape=(256,256,3), activation="relu"))
model.add(MaxPooling2D(pool_size=(3, 3)))
model.add(Conv2D(16, (3, 3), padding="same", activation="relu"))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(8, activation="relu"))
model.add(Dense(3, activation="softmax"))
model.summary()

model.compile(loss = 'categorical_crossentropy', optimizer = Adam(0.0001),metrics=['accuracy'])

#Next we will split the dataset into validation and training data.
# Splitting the training data set into training and validation data sets
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2)

# Training the model
#no.of times the entire training dataset will be passed through the network during training.
epochs = 50
batch_size = 128
history = model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs,
                    validation_data = (x_val, y_val))

#Plot the training history
plt.figure(figsize=(12, 5))
plt.plot(history.history['accuracy'], color='r')
plt.plot(history.history['val_accuracy'], color='b')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.legend(['train', 'val'])

plt.show()

print("[INFO] Calculating model accuracy")
scores = model.evaluate(x_test, y_test)
print(f"Test Accuracy: {scores[1]*100}")

y_pred = model.predict(x_test)

# Plotting image to compare
img = array_to_img(x_test[50])
img

# Finding max value from predition list and comaparing original value vs predicted
print("Original label : ",all_labels[np.argmax(y_test[50])])
print("Predicted label : ",all_labels[np.argmax(y_pred[52])])
print (y_pred[2])

for i in range(50,100):
  print(all_labels[np.argmax(y_test[i])],"-",all_labels[np.argmax(y_pred[i])])
